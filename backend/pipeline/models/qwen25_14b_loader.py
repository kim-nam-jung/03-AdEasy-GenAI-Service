"""
Qwen2.5-14B ëª¨ë¸ ë¡œë” (Step 2 & 1.5ìš© - ì™„ì „ ìµœì í™” ë²„ì „)

âœ¨ í•µì‹¬ ê¸°ëŠ¥:
- 8bit ì–‘ìí™”ë¡œ VRAM ì ˆë°˜ ì‚¬ìš© (15GB â†’ 8GB)
- ë‹¨ì¼ ì œí’ˆë§Œ ì²˜ë¦¬ (product_index ê¸°ë°˜)
- ì‹¤ì‹œê°„ ì§„í–‰ ì‹œê°„ í‘œì‹œ
- ì†ë„ ìµœì í™” (Fast/Creative ëª¨ë“œ)
- Step 1.5: í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­ + í”„ë¡¬í”„íŠ¸ í™•ì¥
- Step 2: 3ì”¬ ì—°ì† AdPlan ìƒì„± (CONTINUE í‚¤ì›Œë“œ ê°•ì œ)
- ë©”ëª¨ë¦¬ ì•ˆì „ ê´€ë¦¬ (unload + GPU ìºì‹œ ì •ë¦¬)

ğŸ“Š ì„±ëŠ¥:
- ë¡œë”©: ~160ì´ˆ (8bit ì–‘ìí™”)
- Step 1.5: ~60ì´ˆ (ë²ˆì—­ + í™•ì¥)
- Step 2: ~70ì´ˆ (Fast ëª¨ë“œ)
- ì´: ~5ë¶„ (ê¸°ì¡´ 15ë¶„ ëŒ€ë¹„ 3ë°° ë¹ ë¦„)

ğŸ”§ v2 ê°œì„ ì‚¬í•­:
- Fast ëª¨ë“œ: max_tokens 384 â†’ 512 (JSON íŒŒì‹± ì•ˆì •í™”)
- Creative ëª¨ë“œ: max_tokens 640 â†’ 768 (ë” ìì„¸í•œ ì„¤ëª…)
"""

import torch
import gc
import json
import time
import re
import threading
from pathlib import Path
from typing import Dict, Any, List, Optional
from transformers import AutoModelForCausalLM, AutoTokenizer


class Qwen25Loader:
    """
    Qwen2.5-14B ë¡œë” (8bit ì–‘ìí™” + ì†ë„ ìµœì í™”)
    
    Usage:
        loader = get_qwen25_loader()
        loader.load()
        
        # Step 1.5: í”„ë¡¬í”„íŠ¸ í™•ì¥
        expanded = loader.expand_prompt(
            user_prompt="ì—¬ë¦„ ì‹œì›í•œ ëŠë‚Œìœ¼ë¡œ",
            description="Blue striped shirt",
            category="shirt",
            color="blue",
            style="casual",
            keywords=["striped", "summer", "beach"]
        )
        
        # Step 2: AdPlan ìƒì„±
        adplan = loader.generate_adplan(
            expanded_prompt=expanded,
            description="Blue striped shirt",
            category="shirt",
            color="blue",
            style="casual",
            keywords=["striped", "summer", "beach"],
            fast_mode=True
        )
        
        loader.unload()  # ë©”ëª¨ë¦¬ í•´ì œ
    """
    
    def __init__(self, model_path: str = "/home/spai0432/ADEASY_SHORTS/models/Qwen2.5-14B"):
        """
        Args:
            model_path: Qwen2.5-14B ëª¨ë¸ ê²½ë¡œ
        """
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸ¯ Qwen2.5-14B Loader initialized")
        print(f"   Device: {self.device}")
        print(f"   Model: {model_path}")
    
    def load(self):
        """
        ëª¨ë¸ ë¡œë“œ (8bit ì–‘ìí™”)
        
        - load_in_8bit=True: 8bit ì–‘ìí™” í™œì„±í™”
        - llm_int8_threshold=6.0: ì–‘ìí™” ì„ê³„ê°’
        - llm_int8_has_fp16_weight=False: INT8ë§Œ ì‚¬ìš© (FP16 í˜¼í•© ë°©ì§€)
        - device_map="auto": ìë™ GPU ë°°ì¹˜
        
        VRAM ì‚¬ìš©ëŸ‰: ~15GB (Qwen2.5-14B ê¸°ì¤€)
        """
        if self.model is not None:
            print("âœ… Model already loaded")
            return
        
        print(f"\nğŸ”„ Loading Qwen2.5-14B (8bit quantized) from {self.model_path}...")
        print("   This will take ~2-3 minutes...")
        start_time = time.time()
        
        try:
            # 1) Tokenizer ë¡œë“œ
            print("   [1/2] Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # 2) ëª¨ë¸ ë¡œë“œ (8bit ì–‘ìí™”)
            print("   [2/2] Loading model (8bit quantized)...")
            print("   âš ï¸  Note: You may see FutureWarnings - this is normal")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                device_map="auto",                  # ìë™ GPU ë°°ì¹˜
                load_in_8bit=True,                  # 8bit ì–‘ìí™” í•µì‹¬!
                llm_int8_threshold=6.0,             # ì–‘ìí™” ì„ê³„ê°’
                llm_int8_has_fp16_weight=False,     # INT8ë§Œ ì‚¬ìš©
                trust_remote_code=True,
                torch_dtype=torch.float16           # ê¸°ë³¸ dtype
            )
            self.model.eval()
            
            elapsed = time.time() - start_time
            print(f"\nâœ… Qwen2.5-14B (8bit) loaded in {elapsed:.1f}s")
            
            # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                print(f"   GPU Memory: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved")
                print(f"   ğŸ’¡ 8bit quantization: FP16 ëŒ€ë¹„ ~50% ì ˆê°")
            
        except ImportError as e:
            print(f"\nâŒ Missing dependencies: {e}")
            print("\nğŸ’¡ Fix:")
            print("   pip install bitsandbytes accelerate")
            raise
        
        except Exception as e:
            print(f"\nâŒ Failed to load model: {e}")
            print("\nğŸ’¡ Troubleshooting:")
            print("   1. Check model path exists:")
            print(f"      ls {self.model_path}")
            print("   2. Install dependencies:")
            print("      pip install bitsandbytes accelerate")
            print("   3. Check GPU availability:")
            print("      python -c 'import torch; print(torch.cuda.is_available())'")
            raise
    
    def unload(self):
        """
        ë©”ëª¨ë¦¬ ì™„ì „ í•´ì œ
        
        - GPU â†’ CPU ì´ë™
        - ê°ì²´ ì‚­ì œ
        - ê°€ë¹„ì§€ ìˆ˜ì§‘
        - GPU ìºì‹œ ì •ë¦¬
        - 2ì´ˆ ëŒ€ê¸° (GPU ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ ë³´ì¥)
        """
        if self.model is None:
            print("âœ… Model already unloaded")
            return
        
        print("\nğŸ—‘ï¸  Unloading Qwen2.5-14B...")
        
        # GPU â†’ CPU
        if self.device == "cuda":
            self.model.cpu()
        
        # ê°ì²´ ì‚­ì œ
        del self.model
        del self.tokenizer
        self.model = None
        self.tokenizer = None
        
        # ê°€ë¹„ì§€ ìˆ˜ì§‘
        gc.collect()
        
        # GPU ìºì‹œ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("   GPU cache cleared")
        
        # ë©”ëª¨ë¦¬ í•´ì œ ëŒ€ê¸°
        time.sleep(2)
        print("âœ… Qwen2.5-14B unloaded successfully")
    
    def _is_english(self, text: str) -> bool:
        """
        í…ìŠ¤íŠ¸ê°€ ì˜ì–´ì¸ì§€ íœ´ë¦¬ìŠ¤í‹± ì²´í¬
        
        Args:
            text: ì²´í¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            True if ì˜ì–´ ë¹„ìœ¨ > 70%, else False
        """
        if not text:
            return True
        
        # ASCII ë¬¸ì ë¹„ìœ¨ ê³„ì‚°
        ascii_count = sum(1 for c in text if ord(c) < 128)
        ratio = ascii_count / len(text)
        
        return ratio > 0.7
    
    def _translate_to_english(self, text: str) -> str:
        """
        ë‹¤êµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­
        
        Qwen2.5ëŠ” ì¤‘êµ­ì–´ ìš°ì„  í•™ìŠµ â†’ í•œêµ­ì–´/ì¤‘êµ­ì–´/ì¼ë³¸ì–´ ì…ë ¥ ì‹œ ì¤‘êµ­ì–´ë¡œ ì‘ë‹µ
        ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì˜ì–´ ì‘ë‹µ ìœ ë„
        
        Args:
            text: ë²ˆì—­í•  í…ìŠ¤íŠ¸ (í•œêµ­ì–´/ì¤‘êµ­ì–´/ì¼ë³¸ì–´)
            
        Returns:
            ì˜ì–´ ë²ˆì—­ ê²°ê³¼
        """
        if self.model is None:
            self.load()
        
        print(f"   ğŸŒ Translating to English: '{text[:50]}...'")
        
        # ë²ˆì—­ í”„ë¡¬í”„íŠ¸
        messages = [
            {
                "role": "system",
                "content": "You are a professional translator. Translate the input to English. Output ONLY the English translation, no explanations."
            },
            {
                "role": "user",
                "content": f"Translate to English:\n{text}"
            }
        ]
        
        # í† í¬ë‚˜ì´ì§•
        text_input = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text_input], return_tensors="pt").to(self.device)
        
        # ìƒì„±
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=128,
                do_sample=False,        # ê²°ì •ì  ë²ˆì—­
                temperature=0.1
            )
        
        # ë””ì½”ë”©
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        translation = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
        
        print(f"   âœ… Translated: '{translation[:50]}...'")
        return translation
    
    def expand_prompt(
        self,
        user_prompt: str,
        description: str,
        category: str,
        color: str,
        style: str,
        keywords: List[str]
    ) -> Dict[str, Any]:
        """
        Step 1.5: ì§§ì€ í”„ë¡¬í”„íŠ¸ â†’ í’ë¶€í•œ ê´‘ê³  ì»¨ì…‰ í™•ì¥
        
        Flow:
            1. í•œêµ­ì–´ ì²´í¬ â†’ ì˜ì–´ ë²ˆì—­
            2. LLMìœ¼ë¡œ 200ë‹¨ì–´ ê´‘ê³  ì»¨ì…‰ ìƒì„±
            3. JSON íŒŒì‹± (original, expanded, keywords, tone, target_audience)
        
        Args:
            user_prompt: ì‚¬ìš©ì ì…ë ¥ (ì˜ˆ: "ì—¬ë¦„ ì‹œì›í•œ ëŠë‚Œìœ¼ë¡œ")
            description: ì œí’ˆ ì„¤ëª…
            category: ì œí’ˆ ì¹´í…Œê³ ë¦¬
            color: ì œí’ˆ ìƒ‰ìƒ
            style: ì œí’ˆ ìŠ¤íƒ€ì¼
            keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            {
                "original": "ì—¬ë¦„ ì‹œì›í•œ ëŠë‚Œìœ¼ë¡œ",
                "original_translated": "Refreshing summer vibe with cool breeze feeling",
                "expanded": "Imagine a breezy summer day at the beach...",
                "keywords": ["beach", "summer", "refreshing", "cool", "casual"],
                "tone": "relaxed and energetic",
                "target_audience": "young adults 20-30s"
            }
        """
        if self.model is None:
            self.load()
        
        print(f"\n{'='*60}")
        print(f"ğŸ¤” Step 1.5: Prompt Expansion")
        print(f"{'='*60}")
        print(f"   Original: '{user_prompt}'")
        
        # 1) í•œêµ­ì–´ â†’ ì˜ì–´ ë²ˆì—­
        original_translated = user_prompt
        if not self._is_english(user_prompt):
            print(f"   ğŸŒ Detected non-English input, translating...")
            original_translated = self._translate_to_english(user_prompt)
        else:
            print(f"   âœ… English input detected")
        
        # 2) í™•ì¥ í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = """You are a creative advertising copywriter.

Task: Expand the user's brief prompt into a rich 200-word advertising concept.

Output JSON format:
{
  "original": "user input",
  "expanded": "detailed 200-word concept with vivid imagery, scenes, and emotional appeal",
  "keywords": ["keyword1", "keyword2", "keyword3", "keyword4", "keyword5"],
  "tone": "mood and emotional tone description",
  "target_audience": "demographic description"
}

Focus on:
1. Season/time of day details
2. Scene/background/location suggestions
3. Emotional tone and mood
4. Target customer profile
5. Visual elements (lighting, colors, camera angles)
6. Brand storytelling

Make it vivid, engaging, and ready for video production."""

        user_message = f"""Product Information:
- Category: {category}
- Color: {color}
- Style: {style}
- Description: {description}
- Keywords: {', '.join(keywords[:5])}

User Request: "{original_translated}"

Generate a rich 200-word advertising concept for this product."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        # í† í¬ë‚˜ì´ì§•
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
        
        # ìƒì„±
        print(f"   ğŸ¨ Expanding prompt to 200-word concept...")
        start_time = time.time()
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=384,         # 200ë‹¨ì–´ ì»¨ì…‰ + JSON êµ¬ì¡°
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
        
        elapsed = time.time() - start_time
        
        # ë””ì½”ë”©
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        print(f"   âœ… Prompt expanded in {elapsed:.1f}s")
        print(f"   ğŸ“ Response length: {len(response)} chars")
        
        # 3) JSON íŒŒì‹±
        try:
            clean_text = response.strip()
            
            # Markdown ì½”ë“œë¸”ë¡ ì œê±°
            if "```json" in clean_text:
                clean_text = clean_text.split("```json")[1].split("```")[0].strip()
            elif "```" in clean_text:
                clean_text = clean_text.split("```")[1].split("```")[0].strip()
            
            result = json.loads(clean_text)
            
            # ë²ˆì—­ë³¸ ì¶”ê°€
            result["original_translated"] = original_translated
            
            # ê²€ì¦
            expanded_len = len(result.get("expanded", "").split())
            print(f"   âœ… Expanded length: {expanded_len} words")
            print(f"   ğŸ¯ Keywords: {', '.join(result.get('keywords', [])[:5])}")
            print(f"   ğŸ­ Tone: {result.get('tone', 'N/A')}")
            
            return result
            
        except json.JSONDecodeError as e:
            print(f"   âš ï¸  JSON parsing failed: {e}")
            print(f"   Using fallback expansion")
            
            # í´ë°±
            return {
                "original": user_prompt,
                "original_translated": original_translated,
                "expanded": f"A {category} advertisement concept featuring {color} tones and {style} style. {original_translated}. The scene captures the essence of the product with engaging visuals and emotional appeal, targeting modern consumers looking for quality and style.",
                "keywords": keywords[:5] if keywords else ["style", "quality", "modern"],
                "tone": "engaging and aspirational",
                "target_audience": "general consumers"
            }
    
    def generate_adplan(
        self,
        expanded_prompt: Dict[str, Any],
        description: str,
        category: str,
        color: str,
        style: str,
        keywords: List[str],
        scene_durations: List[float] = [5.5, 5.0, 5.0],
        fast_mode: bool = False
    ) -> Dict[str, Any]:
        """
        Step 2: í™•ì¥ í”„ë¡¬í”„íŠ¸ â†’ 3-Scene ì—°ì† ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        
        Flow:
            1. í™•ì¥ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
            2. LLMìœ¼ë¡œ 3ì”¬ ì—°ì† ìŠ¤í† ë¦¬ë³´ë“œ ìƒì„± (CONTINUE í‚¤ì›Œë“œ ê°•ì œ)
            3. JSON íŒŒì‹± (scene1, scene2, scene3)
            4. í´ë°± í…œí”Œë¦¿ (íŒŒì‹± ì‹¤íŒ¨ ì‹œ)
        
        Args:
            expanded_prompt: expand_prompt() ê²°ê³¼
            description: ì œí’ˆ ì„¤ëª…
            category: ì œí’ˆ ì¹´í…Œê³ ë¦¬
            color: ì œí’ˆ ìƒ‰ìƒ
            style: ì œí’ˆ ìŠ¤íƒ€ì¼
            keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            scene_durations: ì”¬ ê¸¸ì´ [5.5, 5.0, 5.0]
            fast_mode: True=Fast(512 tokens), False=Creative(768 tokens)
            
        Returns:
            {
                "scene1": {
                    "duration": 5.5,
                    "image_prompt": "Close-up of blue striped shirt on beach...",
                    "video_prompt": "Smooth zoom in revealing fabric details...",
                    "camera_movement": "zoom_in"
                },
                "scene2": {
                    "duration": 5.0,
                    "image_prompt": "CONTINUE from Scene 1, same beach background, shirt in lifestyle context...",
                    "video_prompt": "Gentle pan right showing beach scene...",
                    "camera_movement": "pan_right"
                },
                "scene3": {
                    "duration": 5.0,
                    "image_prompt": "CONTINUE from Scene 2, same beach setting, final product reveal with CTA...",
                    "video_prompt": "Static shot with brand logo appearing...",
                    "camera_movement": "static"
                }
            }
        """
        if self.model is None:
            self.load()
        
        print(f"\n{'='*60}")
        print(f"ğŸ¬ Step 2: AdPlan Generation (3-Scene Continuous Storyboard)")
        print(f"{'='*60}")
        
        # í™•ì¥ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
        expanded_text = expanded_prompt.get("expanded", "")
        tone = expanded_prompt.get("tone", "engaging")
        keywords_expanded = expanded_prompt.get("keywords", keywords[:5])
        
        print(f"   ğŸ“ Concept: {expanded_text[:100]}...")
        print(f"   ğŸ­ Tone: {tone}")
        print(f"   ğŸ¯ Keywords: {', '.join(keywords_expanded)}")
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—°ì†ì„± ê°•ì œ)
        system_prompt = """You are a professional advertising video director.

Task: Create a 3-scene continuous video storyboard with SEAMLESS transitions.

Output JSON format:
{
  "scene1": {
    "duration": 5.5,
    "image_prompt": "detailed scene description with lighting, angle, mood",
    "video_prompt": "camera movement and action description",
    "camera_movement": "zoom_in"
  },
  "scene2": {
    "duration": 5.0,
    "image_prompt": "CONTINUE from Scene 1, same background/location, progressive action",
    "video_prompt": "continuous motion description",
    "camera_movement": "pan_right"
  },
  "scene3": {
    "duration": 5.0,
    "image_prompt": "CONTINUE from Scene 2, same setting, final reveal",
    "video_prompt": "concluding action with call-to-action",
    "camera_movement": "static"
  }
}

**CRITICAL REQUIREMENTS:**
1. Scene 2 MUST start with "CONTINUE from Scene 1, same [background/location]..."
2. Scene 3 MUST start with "CONTINUE from Scene 2, same [setting]..."
3. Use continuity keywords: "same background", "same location", "same setting", "continuous motion", "seamless transition"
4. Maintain consistent lighting, time of day, and environment across all scenes
5. Each scene builds on the previous one

Camera movements (choose one per scene):
- zoom_in: ì ì§„ì  í™•ëŒ€
- zoom_out: ì ì§„ì  ì¶•ì†Œ
- pan_right: ìš°ë¡œ íŒ¨ë‹
- pan_left: ì¢Œë¡œ íŒ¨ë‹
- tilt_up: ìœ„ë¡œ í‹¸íŠ¸
- tilt_down: ì•„ë˜ë¡œ í‹¸íŠ¸
- static: ì •ì  (CTAì— ì í•©)

Storyboard structure:
- Scene 1 (5.5s): Hook - grab attention with striking visual
- Scene 2 (5.0s): Context - show product in use/lifestyle (CONTINUE from Scene 1)
- Scene 3 (5.0s): Call-to-action - brand message/purchase prompt (CONTINUE from Scene 2)"""

        user_message = f"""Product Details:
- Category: {category}
- Color: {color}
- Style: {style}
- Description: {description}

Ad Concept:
{expanded_text}

Tone: {tone}
Keywords: {', '.join(keywords_expanded)}

Generate a CONTINUOUS 3-scene storyboard:
- Scene 1: {scene_durations[0]}s (hook)
- Scene 2: {scene_durations[1]}s (MUST say "CONTINUE from Scene 1")
- Scene 3: {scene_durations[2]}s (MUST say "CONTINUE from Scene 2")

Ensure seamless visual continuity across all scenes."""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        # í† í¬ë‚˜ì´ì§•
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
        
        # ğŸ”§ v2 ê°œì„ : í† í° ìˆ˜ ì¦ê°€ë¡œ JSON íŒŒì‹± ì•ˆì •í™”
        if fast_mode:
            max_tokens = 512         # v1: 384 â†’ v2: 512 (+33%)
            temperature = 0.5
            top_p = 0.85
            mode_name = "ğŸš€ Fast"
        else:
            max_tokens = 768         # v1: 640 â†’ v2: 768 (+20%)
            temperature = 0.7
            top_p = 0.9
            mode_name = "ğŸ¨ Creative"
        
        print(f"   {mode_name} mode (max_tokens={max_tokens})")
        print(f"   ğŸ¬ Generating continuous storyboard...")
        
        start_time = time.time()
        
        # ì§„í–‰ ì‹œê°„ ì¶œë ¥ ìŠ¤ë ˆë“œ
        stop_timer = threading.Event()
        def print_elapsed():
            while not stop_timer.is_set():
                elapsed = time.time() - start_time
                print(f"   â±ï¸  Elapsed: {elapsed:.0f}s...", end='\r', flush=True)
                time.sleep(2)
        
        timer_thread = threading.Thread(target=print_elapsed, daemon=True)
        timer_thread.start()
        
        # ìƒì„±
        try:
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **model_inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=temperature,
                    top_p=top_p,
                    repetition_penalty=1.1
                )
        finally:
            stop_timer.set()
            timer_thread.join()
        
        elapsed = time.time() - start_time
        print(f"\n   âœ… Completed in {elapsed:.1f}s                    ")
        
        # ë””ì½”ë”©
        generated_ids = [
            output_ids[len(input_ids):] 
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        print(f"   ğŸ“ Response length: {len(response)} chars")
        
        # JSON íŒŒì‹±
        adplan = self._parse_adplan(response, scene_durations)
        
        # ì—°ì†ì„± ê²€ì¦
        self._validate_continuity(adplan)
        
        return adplan
    
    def _parse_adplan(self, text: str, durations: List[float]) -> Dict[str, Any]:
        """
        LLM ì‘ë‹µ â†’ AdPlan JSON íŒŒì‹±
        
        Args:
            text: LLM ìƒì„± í…ìŠ¤íŠ¸
            durations: ì”¬ ê¸¸ì´ [5.5, 5.0, 5.0]
            
        Returns:
            AdPlan ë”•ì…”ë„ˆë¦¬ (scene1, scene2, scene3)
        """
        try:
            clean_text = text.strip()
            
            # Markdown ì½”ë“œë¸”ë¡ ì œê±°
            if "```json" in clean_text:
                clean_text = clean_text.split("```json")[1].split("```")[0].strip()
            elif "```" in clean_text:
                clean_text = clean_text.split("```")[1].split("```")[0].strip()
            
            # JSON íŒŒì‹±
            adplan = json.loads(clean_text)
            
            # ì”¬ ê²€ì¦ ë° ë³´ì™„
            for i, scene_key in enumerate(["scene1", "scene2", "scene3"], 1):
                if scene_key not in adplan:
                    print(f"   âš ï¸  Missing {scene_key}, using fallback")
                    adplan[scene_key] = self._fallback_scene(i, durations[i-1])
                else:
                    # duration ê°•ì œ ì„¤ì •
                    adplan[scene_key]["duration"] = durations[i-1]
                    
                    # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                    required_fields = ["image_prompt", "video_prompt", "camera_movement"]
                    for field in required_fields:
                        if field not in adplan[scene_key] or not adplan[scene_key][field]:
                            print(f"   âš ï¸  {scene_key}.{field} missing, using fallback")
                            fallback = self._fallback_scene(i, durations[i-1])
                            adplan[scene_key][field] = fallback[field]
            
            print(f"   âœ… AdPlan parsed successfully")
            return adplan
            
        except json.JSONDecodeError as e:
            print(f"   âš ï¸  JSON parsing failed: {e}")
            print(f"   Using fallback AdPlan templates")
            return self._fallback_adplan(text, durations)
    
    def _fallback_scene(self, scene_id: int, duration: float) -> Dict[str, Any]:
        """
        í´ë°± Scene í…œí”Œë¦¿ (ì—°ì†ì„± í‚¤ì›Œë“œ í¬í•¨)
        
        Args:
            scene_id: ì”¬ ë²ˆí˜¸ (1, 2, 3)
            duration: ì”¬ ê¸¸ì´ (ì´ˆ)
            
        Returns:
            Scene ë”•ì…”ë„ˆë¦¬
        """
        templates = {
            1: {
                "duration": duration,
                "image_prompt": "Close-up product showcase with dramatic lighting, shallow depth of field, professional studio setup",
                "video_prompt": "Smooth zoom in revealing intricate product details and textures",
                "camera_movement": "zoom_in"
            },
            2: {
                "duration": duration,
                "image_prompt": "CONTINUE from Scene 1, same background lighting, product placed in lifestyle context with soft natural light",
                "video_prompt": "Gentle pan right showing product in everyday use, continuous motion from previous scene",
                "camera_movement": "pan_right"
            },
            3: {
                "duration": duration,
                "image_prompt": "CONTINUE from Scene 2, same setting and mood, final product reveal with brand logo and call-to-action text overlay",
                "video_prompt": "Static shot allowing viewer to absorb final message, seamless conclusion from previous scenes",
                "camera_movement": "static"
            }
        }
        
        return templates.get(scene_id, templates[1])
    
    def _fallback_adplan(self, text: str, durations: List[float]) -> Dict[str, Any]:
        """
        ì™„ì „ í´ë°± AdPlan (JSON íŒŒì‹± ì™„ì „ ì‹¤íŒ¨ ì‹œ)
        
        Args:
            text: ì‹¤íŒ¨í•œ LLM ì‘ë‹µ
            durations: ì”¬ ê¸¸ì´
            
        Returns:
            ì™„ì „ í´ë°± AdPlan
        """
        print("   ğŸ“‹ Generating fallback AdPlan with continuity keywords...")
        
        return {
            "scene1": self._fallback_scene(1, durations[0]),
            "scene2": self._fallback_scene(2, durations[1]),
            "scene3": self._fallback_scene(3, durations[2])
        }
    
    def _validate_continuity(self, adplan: Dict[str, Any]) -> None:
        """
        AdPlan ì—°ì†ì„± ê²€ì¦
        
        Scene 2/3ì˜ image_promptì— "CONTINUE" í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ ì²´í¬
        """
        continuity_keywords = ["continue", "same background", "same location", "same setting", "continuous"]
        
        for scene_id in [2, 3]:
            scene_key = f"scene{scene_id}"
            image_prompt = adplan.get(scene_key, {}).get("image_prompt", "").lower()
            
            has_continuity = any(keyword in image_prompt for keyword in continuity_keywords)
            
            if has_continuity:
                print(f"   âœ… Scene {scene_id}: Continuity keywords found")
            else:
                print(f"   âš ï¸  Scene {scene_id}: Missing continuity keywords (may cause visual jumps)")


# ========================================
# ì‹±ê¸€í†¤ íŒ¨í„´
# ========================================

_qwen25_loader_instance = None

def get_qwen25_loader() -> Qwen25Loader:
    """
    ì‹±ê¸€í†¤ Qwen25Loader ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Usage:
        loader = get_qwen25_loader()
        loader.load()
        # ... use loader ...
        loader.unload()
    
    Returns:
        Qwen25Loader ì¸ìŠ¤í„´ìŠ¤
    """
    global _qwen25_loader_instance
    
    if _qwen25_loader_instance is None:
        _qwen25_loader_instance = Qwen25Loader()
    
    return _qwen25_loader_instance


# ========================================
# CLI í…ŒìŠ¤íŠ¸ (standalone ì‹¤í–‰)
# ========================================

if __name__ == "__main__":
    """
    Standalone í…ŒìŠ¤íŠ¸
    
    Usage:
        python qwen25_14b_loader.py
    """
    print("="*60)
    print("Qwen2.5-14B Loader Test (8bit Optimized - v2)")
    print("="*60)
    
    # ë¡œë” ìƒì„±
    loader = get_qwen25_loader()
    
    try:
        # ë¡œë“œ
        loader.load()
        
        # Step 1.5: í”„ë¡¬í”„íŠ¸ í™•ì¥
        print("\n" + "="*60)
        print("Testing Step 1.5: Prompt Expansion")
        print("="*60)
        
        expanded = loader.expand_prompt(
            user_prompt="ë°ê³  ì²­ëŸ‰í•œ ëŠë‚Œìœ¼ë¡œ ê´‘ê³ í•´ ì¤„ ê²ƒ",
            description="Blue striped shirt with chest pocket",
            category="shirt",
            color="blue",
            style="casual",
            keywords=["striped", "pockets", "long-sleeve", "cotton", "beach"]
        )
        
        print("\nğŸ“Š Expanded Prompt:")
        print(json.dumps(expanded, indent=2, ensure_ascii=False))
        
        # Step 2: AdPlan ìƒì„±
        print("\n" + "="*60)
        print("Testing Step 2: AdPlan Generation")
        print("="*60)
        
        adplan = loader.generate_adplan(
            expanded_prompt=expanded,
            description="Blue striped shirt with chest pocket",
            category="shirt",
            color="blue",
            style="casual",
            keywords=["striped", "pockets", "long-sleeve", "cotton", "beach"],
            fast_mode=True
        )
        
        print("\nğŸ“Š AdPlan:")
        for scene_key in ["scene1", "scene2", "scene3"]:
            scene = adplan[scene_key]
            print(f"\n{scene_key.upper()}:")
            print(f"  Duration: {scene['duration']}s")
            print(f"  Camera: {scene['camera_movement']}")
            print(f"  Image: {scene['image_prompt'][:80]}...")
            print(f"  Video: {scene['video_prompt'][:80]}...")
        
        print("\n" + "="*60)
        print("âœ… Test completed successfully!")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ì–¸ë¡œë“œ
        loader.unload()