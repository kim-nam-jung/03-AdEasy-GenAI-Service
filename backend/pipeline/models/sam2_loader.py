# pipeline/models/sam2_loader.py
"""
SAM 2 ëª¨ë¸ ë¡œë” (Box Prompt ë°©ì‹ ì ìš©)
- ì „ëµ: ì (Point) ëŒ€ì‹  ì¤‘ì•™ ì˜ì—­ ë°•ìŠ¤(Box)ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©
- í•´ê²°: 'í† ë§ˆí† 'ë§Œ ë”°ëŠ” ë¬¸ì œ í•´ê²° -> ë°•ìŠ¤ ë‚´ë¶€ì˜ 'ì „ì²´ ê°ì²´' ì¸ì‹ ìœ ë„
- íš¨ê³¼: ì£¼ë³€ë¶€(ê°ìíŠ€ê¹€, ì ‘ì‹œ ë) ë°°ì œ ë° ë©”ì¸ ê°ì²´(í–„ë²„ê±°) ì „ì²´ í¬ì°©
"""

from ultralytics import SAM
from pathlib import Path
import torch
import cv2
import numpy as np
from PIL import Image

class SAM2Loader:
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super(SAM2Loader, cls).__new__(cls)
            cls._instance.model = None
            cls._instance.device = "cuda" if torch.cuda.is_available() else "cpu"
            cls._instance.model_path = "sam2.1_l.pt" 
        return cls._instance

    def load(self):
        if self.model is None:
            print(f"ğŸ”„ Loading SAM 2 (Large) from {self.model_path}...")
            try:
                self.model = SAM(self.model_path)
            except Exception as e:
                print(f"âš ï¸ Error loading SAM 2: {e}")
                raise
            self.model.to(self.device)
            print(f"âœ… SAM 2 loaded on {self.device}")
        return self.model
    
    def unload(self):
        if self.model is not None:
            del self.model
            self.model = None
            torch.cuda.empty_cache()
            print("ğŸ—‘ï¸ SAM 2 unloaded")
    
    def segment(self, image_path: str, output_dir: str, conf: float = None, iou: float = None):
        """
        SAM 2 ë°•ìŠ¤ í”„ë¡¬í”„íŠ¸ ì„¸ê·¸ë©˜í…Œì´ì…˜
        """
        model = self.load()
        
        # 1. ì´ë¯¸ì§€ ì •ë³´ ì½ê¸°
        original = cv2.imread(image_path)
        if original is None:
             raise FileNotFoundError(f"Image not found: {image_path}")
             
        orig_h, orig_w = original.shape[:2]
        
        # âœ¨ ì „ëµ ìˆ˜ì •: ì¤‘ì•™ "ë°•ìŠ¤(Box)" í”„ë¡¬í”„íŠ¸ ìƒì„±
        # ì´ë¯¸ì§€ì˜ ì¤‘ì•™ 50% ì •ë„ë¥¼ ì»¤ë²„í•˜ëŠ” ë°•ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        # ë„ˆë¬´ ì‘ìœ¼ë©´ í† ë§ˆí† ë§Œ ì¡ê³ , ë„ˆë¬´ í¬ë©´ ê°ìíŠ€ê¹€ê¹Œì§€ ì¡ìœ¼ë¯€ë¡œ 0.25~0.75 êµ¬ê°„(50%)ì´ ì ë‹¹í•©ë‹ˆë‹¤.
        
        margin_x = int(orig_w * 0.20) # ì¢Œìš° 20%ì”© ì—¬ë°± (ì¤‘ì•™ 60% ì‚¬ìš©)
        margin_y = int(orig_h * 0.20) # ìƒí•˜ 20%ì”© ì—¬ë°±
        
        x1 = margin_x
        y1 = margin_y
        x2 = orig_w - margin_x
        y2 = orig_h - margin_y
        
        box_prompt = [x1, y1, x2, y2] # [x1, y1, x2, y2]
        
        print(f"ğŸ¯ SAM 2 Box Prompting: {box_prompt} (Center Focused)")

        # 2. ì¶”ë¡  ì‹¤í–‰ (ë°•ìŠ¤ í”„ë¡¬í”„íŠ¸)
        # bboxes ì¸ìë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        results = model(
            image_path,
            bboxes=[box_prompt],
            device=self.device,
            retina_masks=True,
            verbose=False
        )
        
        if not results or not results[0].masks:
            print("âš ï¸ SAM 2 found no objects in the box.")
            return {"main_image": image_path, "mask": None}

        # SAM 2ëŠ” ë³´í†µ 3ê°œì˜ ë§ˆìŠ¤í¬ í›„ë³´ë¥¼ ì¤ë‹ˆë‹¤ (Small, Medium, Large ëŠë‚Œ)
        # ìš°ë¦¬ëŠ” ì´ ì¤‘ì—ì„œ "ë°•ìŠ¤ í¬ê¸°ì™€ ê°€ì¥ ë¹„ìŠ·í•œ" ë†ˆì„ ê³¨ë¼ì•¼ í•©ë‹ˆë‹¤.
        # ë³´í†µ ë§ˆì§€ë§‰(index 2)ì´ë‚˜ ì¤‘ê°„(index 1)ì´ ì „ì²´ ê°ì²´ì¼ í™•ë¥ ì´ ë†’ìŠµë‹ˆë‹¤.
        
        masks = results[0].masks.data # (3, H, W) usually
        best_mask = None
        best_score = -float('inf')

        # ë°•ìŠ¤ ë©´ì  ê³„ì‚°
        box_area = (x2 - x1) * (y2 - y1)

        for i, mask_tensor in enumerate(masks):
            mask_np = mask_tensor.cpu().numpy()
            mask_resized = cv2.resize(mask_np.astype(np.float32), (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)
            
            mask_area = mask_resized.sum()
            
            # ì ìˆ˜ ì‚°ì • ë¡œì§:
            # 1. ë°•ìŠ¤ ë©´ì ê³¼ ë§ˆìŠ¤í¬ ë©´ì ì´ ë¹„ìŠ·í• ìˆ˜ë¡ ì¢‹ìŒ (ë„ˆë¬´ ì‘ìœ¼ë©´ í† ë§ˆí† , ë„ˆë¬´ í¬ë©´ ë°°ê²½)
            # 2. í™”ë©´ ì „ì²´ë¥¼ ë®ìœ¼ë©´ ì•ˆë¨ (> 90%)
            
            img_area = orig_w * orig_h
            coverage = mask_area / img_area
            
            # ë„ˆë¬´ í° ë°°ê²½(90% ì´ìƒ)ì€ ì œì™¸
            if coverage > 0.90:
                continue
                
            # ë„ˆë¬´ ì‘ì€ íŒŒí¸(5% ë¯¸ë§Œ)ì€ ì œì™¸
            if coverage < 0.05:
                continue

            # ë°•ìŠ¤ ë©´ì  ëŒ€ë¹„ ë¹„ìœ¨ (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë°•ìŠ¤ë¥¼ ê½‰ ì±„ìš´ ê²ƒ)
            fill_ratio = mask_area / box_area
            
            # ì ìˆ˜ = fill_ratio (ë‹¨, 1.2ë°° ì´ìƒ ë„˜ì–´ê°€ë©´ ê°ì  - ë°•ìŠ¤ ë°–ìœ¼ë¡œ ë§ì´ ë‚˜ê°”ë‹¤ëŠ” ëœ»)
            if fill_ratio > 1.5:
                score = 1.0 / fill_ratio # íŒ¨ë„í‹°
            else:
                score = fill_ratio
            
            # SAM ìì²´ confidence scoreê°€ ìˆë‹¤ë©´ í™œìš© (results[0].box.conf ê°™ì€ ê²ƒì´ ìˆì„ ìˆ˜ ìˆìŒ)
            # ì—¬ê¸°ì„œëŠ” ë©´ì  ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
            
            print(f"  Candidate {i}: Coverage={coverage:.2f}, BoxFill={fill_ratio:.2f} -> Score={score:.2f}")

            if score > best_score:
                best_score = score
                best_mask = mask_resized

        # ë§Œì•½ ì ì ˆí•œ í›„ë³´ë¥¼ ëª» ì°¾ì•˜ë‹¤ë©´, ê°€ì¥ í° ë§ˆìŠ¤í¬ ì„ íƒ (Fallback)
        if best_mask is None:
             best_mask = masks[0].cpu().numpy()
             best_mask = cv2.resize(best_mask.astype(np.float32), (orig_w, orig_h))

        # âœ¨ í›„ì²˜ë¦¬: Connected Components (ë§ˆìŠ¤í¬ íŒŒí¸ ì œê±°)
        mask_uint8 = (best_mask > 0.5).astype(np.uint8)
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_uint8, connectivity=8)
        
        if num_labels > 2: 
            # ì¤‘ì•™ì (í˜¹ì€ ë°•ìŠ¤ ì¤‘ì‹¬)ì„ í¬í•¨í•˜ëŠ” ë©ì–´ë¦¬ ì°¾ê¸°
            center_x, center_y = orig_w // 2, orig_h // 2
            center_label = labels[center_y, center_x]
            
            if center_label != 0:
                product_mask = (labels == center_label).astype(np.float32)
            else:
                # ì¤‘ì‹¬ì´ ë¹„ì—ˆìœ¼ë©´ ê°€ì¥ í° ë©ì–´ë¦¬ ì„ íƒ
                largest_comp_idx = stats[1:, cv2.CC_STAT_AREA].argmax() + 1
                product_mask = (labels == largest_comp_idx).astype(np.float32)
        else:
            product_mask = best_mask.astype(np.float32)

        # --- ì €ì¥ ---
        original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)
        mask_binary = (product_mask > 0.5).astype(np.float32)
        
        mask_3ch = np.stack([mask_binary]*3, axis=-1)
        main_image_rgb = (original_rgb.astype(np.float32) * mask_3ch).astype(np.uint8)
        
        mask_255 = (mask_binary * 255).astype(np.uint8)
        rgba = np.dstack([main_image_rgb, mask_255])
        
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        filename = Path(image_path).stem
        
        save_path = output_dir / f"{filename}_seg.png"
        mask_save_path = output_dir / f"{filename}_mask.png"
        
        Image.fromarray(rgba, mode='RGBA').save(save_path)
        Image.fromarray(mask_255, mode='L').save(mask_save_path)
        
        return {"main_image": str(save_path), "mask": str(mask_save_path)}

def get_sam2_loader():
    return SAM2Loader()