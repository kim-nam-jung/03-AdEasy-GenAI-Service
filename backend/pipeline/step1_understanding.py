# pipeline/step1_understanding.py
"""
Step 1: ì œí’ˆ ì´í•´ ë° ê¸°íš ì´ˆì•ˆ (Product Understanding & Prompt Augmentation)
- ìž…ë ¥: ëˆ„ë¼ ì´ë¯¸ì§€ + ì‚¬ìš©ìž ìš”ì²­(Raw Prompt)
- ëª¨ë¸: GPT-4o (Vision)
- ê¸°ëŠ¥: 
  1. ì‚¬ìš©ìž í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë¬¸ì ì¸ ë¹„ë””ì˜¤ ìƒì„±ìš© ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ì¦ê°• (Augmentation)
  2. í–¥í›„ ì¼ê´€ì„± ìœ ì§€ë¥¼ ìœ„í•œ ì‹œê°ì  DNA(Visual Features) ì¶”ì¶œ
  3. ê´‘ê³  ë¶„ìœ„ê¸° ë° í‚¤ì›Œë“œ ë¶„ì„
"""

import os
import base64
import json
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
from common.logger import get_logger
from common.paths import TaskPaths
from common.config import Config
from common.logger import TaskLogger

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
logger = get_logger("Step1_Understanding")

class Step1_Understanding:
    def __init__(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("OPENAI_API_KEY not found.")
            raise ValueError("OPENAI_API_KEY is missing.")
        
        self.client = OpenAI(api_key=api_key)

    def _encode_image(self, image_path):
        """ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def run(self, task_id: str, image_path: str, user_prompt: str = "") -> dict:
        """
        GPT-4o Visionì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ì„ ë° í”„ë¡¬í”„íŠ¸ ì¦ê°• ìˆ˜í–‰
        """
        logger.info(f"ðŸ§ [Step 1] Analyzing & Augmenting: {Path(image_path).name}")
        
        try:
            # 1. ì´ë¯¸ì§€ ì¸ì½”ë”©
            base64_image = self._encode_image(image_path)
            
            # 2. ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Expert Video Director Persona)
            system_prompt = """
            You are an expert AI Video Director and Creative Strategist.
            Your task is to analyze the product image (transparent background) and the user's request to prepare for a high-quality video advertisement.

            **GOALS:**
            1. **Understand:** Identify the product and its visual details perfectly.
            2. **Augment:** Convert the user's simple request into a **Professional Video Generation Prompt** (English).
            3. **Consistency:** Extract "Visual DNA" to ensure the same product look in future generation steps.

            **OUTPUT FORMAT (JSON):**
            {
                "main_object": "Short name of the product",
                "visual_dna": "Detailed description of the product's physical appearance ONLY (colors, textures, shape, ingredients). This will be used to maintain consistency in image generation.",
                "mood_atmosphere": "The vibe of the video (e.g., Cinematic, Energetic, Fresh, Luxury)",
                "augmented_video_prompt": "A highly detailed, cinematic English prompt for video generation models (like Runway/Sora). Incorporate camera movement, lighting, style, and the user's intent. Start with 'Cinematic shot of...'",
                "ad_keywords": ["Keyword1", "Keyword2", "Keyword3", "Keyword4", "Keyword5"]
            }
            """

            user_content = f"User Request: {user_prompt}" if user_prompt else "Make a cool commercial for this product."

            # 3. GPT-4o í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system", 
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": user_content},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
                        ],
                    }
                ],
                response_format={"type": "json_object"},
                max_tokens=800, # ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•´ í† í° ìˆ˜ ì¦ê°€
                temperature=0.7
            )

            # 4. ê²°ê³¼ íŒŒì‹±
            analysis_result = json.loads(response.choices[0].message.content)
            
            # ë¡œê·¸ì— ì¦ê°•ëœ í”„ë¡¬í”„íŠ¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            logger.info(f"âœ¨ Augmented Prompt: {analysis_result.get('augmented_video_prompt')}")
            
            return analysis_result

        except Exception as e:
            logger.error(f"âŒ Step 1 Analysis Failed: {e}")
            raise e

# ==================== Adapter Function ====================
def step1_understanding(
    task_id: str,
    paths: TaskPaths,
    logger: TaskLogger,
    cfg: Config,
    image_paths: list,
    prompt: str = "",
    **kwargs
) -> dict:
    """
    Step 1 Adapter
    First image is considered main product for analysis.
    """
    # Use the first image for understanding
    if not image_paths:
        raise ValueError("No image paths provided for Step 1")
    
    main_image_path = image_paths[0]
    
    step1 = Step1_Understanding()
    # Ensure correct user prompt is passed
    result = step1.run(task_id, main_image_path, user_prompt=prompt)
    
    return result