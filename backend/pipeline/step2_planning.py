# pipeline/step2_planning.py
"""
Step 2: ê´‘ê³  ì‹œë‚˜ë¦¬ì˜¤ ê¸°íš (Scenario Planning)
- ìž…ë ¥: ëˆ„ë¼ ì´ë¯¸ì§€ + Step 1 ë¶„ì„ ë°ì´í„° (JSON) + User Prompt
- ëª¨ë¸: GPT-4o (Vision)
- ê¸°ëŠ¥: 
  1. 3ë‹¨ êµ¬ì„± (Intro -> Body -> Outro) ì‹œë‚˜ë¦¬ì˜¤ ìž‘ì„±
  2. **ì˜ìƒ ì—°ì†ì„±(Continuity)** ì„¤ê³„: ì´ì „ ìž¥ë©´ì˜ ëì´ ë‹¤ìŒ ìž¥ë©´ì˜ ì‹œìž‘ì´ ë˜ë„ë¡ ê¸°íš
  3. **Start/End Frame Description ì¶”ê°€**: Step 4/5ì—ì„œ í™œìš©í•  í”„ë ˆìž„ ì—°ê²° ì •ë³´
  4. **ë²”ìš©ì„± ê°•í™”**: ìŒì‹, íŒ¨ì…˜, í…Œí¬ ë“± ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€ì‘í•˜ëŠ” ì ì‘í˜• ì—°ì¶œ
"""

import os
import base64
import json
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
from common.logger import get_logger
from common.paths import TaskPaths
from common.config import Config
from common.logger import TaskLogger
from common.schema import AdPlan

load_dotenv()
logger = get_logger("Step2_Planning")

class Step2_Planning:
    def __init__(self):
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY is missing.")
        self.client = OpenAI(api_key=api_key)

    def _encode_image(self, image_path):
        """ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©"""
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def run(self, task_id: str, image_path: str, analysis_data: dict, user_prompt: str = "") -> dict:
        """
        3-Scene ê´‘ê³  ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (ì—°ì†ì„± + í”„ë ˆìž„ ì—°ê²° ì •ë³´ í¬í•¨)
        """
        logger.info(f"ðŸ“ [Step 2] Planning Scenario based on analysis...")
        
        try:
            base64_image = self._encode_image(image_path)
            
            # Step 1 ë°ì´í„° ì •ë¦¬
            product_name = analysis_data.get('main_object', 'Product')
            mood = analysis_data.get('mood_atmosphere', 'Cinematic')
            visual_dna = analysis_data.get('visual_dna', '')
            augmented_prompt = analysis_data.get('augmented_video_prompt', '')

            # [í•µì‹¬] í”„ë ˆìž„ ì—°ê²° ì •ë³´ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_prompt = """
            You are a world-class AI Commercial Director capable of directing ads for ANY product category (Food, Fashion, Tech, Beauty, etc.).
            Create a highly engaging 15-second video advertisement plan (3 Scenes) for the provided product image and details.

            **1. ADAPTIVE DIRECTING STRATEGY (Category Detection):**
            - Analyze the product type first.
            - If **Food**: Focus on appetite appeal, steam, texture, freshness (Sizzle).
            - If **Fashion**: Focus on fabric flow, model movement, fit, style (Chic).
            - If **Tech/Gadget**: Focus on sleek lines, lighting reflections, futuristic mood (Premium).
            - If **Beauty**: Focus on texture smoothness, glow, purity (Elegant).

            **2. CRITICAL REQUIREMENT: VISUAL CONTINUITY (Chain-of-Visuals)**
            - The video will be generated by AI (Image-to-Video).
            - **The End Frame of Scene 1 MUST conceptually match the Start Frame of Scene 2.**
            - Describe the transition clearly using **start_frame_description** and **end_frame_description**.

            **3. FRAME-TO-FRAME CONNECTION (NEW):**
            - **start_frame_description**: What the first frame of this scene looks like (for Scene 1, this is the original product image).
            - **end_frame_description**: What the last frame of this scene should look like (this becomes the start frame of the next scene).
            - **transition_to_next**: How the camera/subject moves to connect to the next scene.

            **4. STRUCTURE (15s Total):**
            - **Scene 1 (Intro, 0-5s):** The "Hook". Strong visual impact to grab attention.
            - **Scene 2 (Body, 5-10s):** The "Experience". Show the product in action or highlight key features.
            - **Scene 3 (Outro, 10-15s):** The "Climax & CTA". Final impressive shot with a call-to-action.

            **OUTPUT JSON FORMAT:**
            {
                "concept": "Creative Concept Title",
                "target_audience": "Target demographic",
                "mood": "Atmosphere description",
                "scenes": [
                    {
                        "scene_id": 1,
                        "duration": 5.0,
                        "description": "Visual description of what happens.",
                        "camera_movement": "e.g., Zoom In, Pan Right",
                        "start_frame_description": "What the first frame looks like",
                        "end_frame_description": "What the last frame looks like",
                        "keyframe_prompt_image": "Stable Diffusion Prompt for generating the keyframe",
                        "keyframe_prompt_video": "Video generation prompt"
                    },
                    {
                        "scene_id": 2,
                        "duration": 5.0,
                        "description": "...",
                        "camera_movement": "...",
                        "start_frame_description": "...",
                        "end_frame_description": "...",
                        "keyframe_prompt_image": "...",
                        "keyframe_prompt_video": "..."
                    },
                    {
                        "scene_id": 3,
                        "duration": 5.0,
                        "description": "...",
                        "camera_movement": "...",
                        "start_frame_description": "...",
                        "end_frame_description": "...",
                        "keyframe_prompt_image": "...",
                        "keyframe_prompt_video": "..."
                    }
                ]
            }
            """

            user_content = f"""
            **Product Name:** {product_name}
            **Mood:** {mood}
            **Visual Features (DNA):** {visual_dna}
            **Augmented Prompt:** {augmented_prompt}
            **User Request:** {user_prompt if user_prompt else "No specific request"}
            
            Please plan a 3-scene video with clear start/end frame descriptions for seamless transitions.
            """

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system", 
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": user_content},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}
                        ],
                    }
                ],
                response_format={"type": "json_object"},
                max_tokens=1500,
                temperature=0.7
            )

            plan_result = json.loads(response.choices[0].message.content)
            logger.info("âœ… Scenario planning completed with frame descriptions.")
            
            return plan_result

        except Exception as e:
            logger.error(f"âŒ Step 2 Planning Failed: {e}")
            raise e

# ==================== Adapter Function ====================
def step2_planning(
    task_id: str,
    paths: TaskPaths,
    logger: TaskLogger,
    cfg: Config,
    prompt: str = "",
    processed_images: list = None,
    analysis_result: dict = None, # Passed as step1_result in orchestrator?
    **kwargs
) -> dict:
    """
    Step 2 Adapter
    """
    # Orchestrator passes:
    # prompt=self.prompt,
    # **result -> contains 'processed_images', 'analysis_result' (from step1_understanding return??)
    
    # Step 1 returns `analysis_result` directly?
    # Step 1 adapter returns `result`. Orchestrator does `result.update(step1_adapter_result)`.
    # So `result` contains the keys from Step 1's return dict.
    # Step 1 returns `analysis_result` dict directly.
    # So `analysis_result` keys ('main_object', etc.) are merged into `result`.
    # But we need the whole analysis dict.
    # Or Orchestrator passes `step1_result` explicitly?
    
    # Wait, Orchestrator Step 1 code:
    # step1_result = step1_understanding(...)
    # result.update(step1_result)
    
    # Step 2 Adapter Args in Orchestrator (lines 174-182):
    # step2_planning(..., prompt=self.prompt, **result)
    
    # So `analysis_result` is NOT passed explicitly as a dict named 'analysis_result'.
    # It is passed as unpacked `**result`, so kwargs contains 'main_object', 'visual_dna', etc.
    
    # So `kwargs` has Step 1 outputs.
    # Also `processed_images` is in `result` (from Step 0).
    
    if processed_images and len(processed_images) > 0:
        main_image = processed_images[0]
    else:
        # Fallback provided by caller or error
        raise ValueError("Step 2 requires processed images")

    # Reconstruct analysis data from kwargs
    analysis_data = {
        "main_object": kwargs.get("main_object"),
        "visual_dna": kwargs.get("visual_dna"),
        "mood_atmosphere": kwargs.get("mood_atmosphere"),
        "augmented_video_prompt": kwargs.get("augmented_video_prompt"),
        "ad_keywords": kwargs.get("ad_keywords")
    }

    planner = Step2_Planning()
    plan_json = planner.run(task_id, main_image, analysis_data, user_prompt=prompt)
    
    # Convert to AdPlan Object (Pydantic)
    try:
        adplan = AdPlan(**plan_json)
    except Exception as e:
        logger.warning(f"AdPlan validation failed, using raw dict as fallback? Error: {e}")
        # Try to patch or re-raise. Pydantic is strict.
        # We must ensure GPT returns correct fields matching AdPlan schema.
        # AdPlan schema requires: concept, target_audience, mood, scenes, etc.
        # My system prompt output JSON format matches AdPlan (mostly).
        # scenes list items match ScenePlan?
        # ScenePlan requires: scene_id, duration, keyframe_prompt_image, keyframe_prompt_video
        raise e

    return {
        "adplan": adplan,
        "scenario": plan_json # Keep raw json too if needed
    }